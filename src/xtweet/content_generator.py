"""
ê¸ˆìœµ/íˆ¬ì ì „ë¬¸ ì½˜í…ì¸  ìƒì„± ì—”ì§„
"""

import logging
import re
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple

from .app import db
from .models import ScrapedContent, GeneratedContent
from .content_templates import get_template, classify_content_type


logger = logging.getLogger(__name__)


class FinancialContentGenerator:
    """ê¸ˆìœµ/íˆ¬ì ì „ë¬¸ ì½˜í…ì¸  ìƒì„±ê¸° - ê°œì„ ëœ ë²„ì „"""
    
    def __init__(self):
        self.max_length = 2000  # ë” ìƒì„¸í•œ ë¶„ì„ì„ ìœ„í•´ ì¦ê°€
        self.min_length = 150
        
    def generate_from_scraped_content(self, scraped_content: ScrapedContent) -> Optional[GeneratedContent]:
        """ìŠ¤í¬ë˜í•‘ëœ ì½˜í…ì¸ ë¡œë¶€í„° ê°œì„ ëœ êµ¬ì¡°í™”ëœ ê¸ˆìœµ ì½˜í…ì¸  ìƒì„±"""
        try:
            logger.info(f"ê°œì„ ëœ êµ¬ì¡°í™”ëœ ê¸ˆìœµ ì½˜í…ì¸  ìƒì„± ì‹œì‘: {scraped_content.title}")
            
            # ì½˜í…ì¸  ì •ì œ ë° ë¶„ì„
            korean_title = self._translate_title(str(scraped_content.title))
            cleaned_content = self._clean_and_analyze_content(str(scraped_content.content))
            
            # ê°œì„ ëœ ê¸ˆìœµ ì „ë¬¸ ìš”ì•½ ìƒì„±
            financial_summary = self._generate_enhanced_financial_summary(cleaned_content)
            
            # ì •ëŸ‰ì  ê¸ˆìœµ íƒœê·¸ ì¶”ì¶œ
            financial_tags = self._extract_quantitative_financial_tags(cleaned_content, korean_title)
            
            # ìƒˆë¡œìš´ ê°œì„ ëœ êµ¬ì¡°í™”ëœ í…œí”Œë¦¿ ì‚¬ìš©
            from .content_templates import StructuredFinancialTemplate
            
            formatted_content = StructuredFinancialTemplate.format_content(
                title=korean_title,
                summary=financial_summary,
                url=str(scraped_content.url) if scraped_content.url else None,
                source=scraped_content.target.name if hasattr(scraped_content, 'target') and scraped_content.target else None,
                tags=financial_tags
            )
            
            # ìƒì„±ëœ ì½˜í…ì¸  ì €ì¥
            generated_content = GeneratedContent(
                scraped_content_id=scraped_content.id,
                title=korean_title,
                content=formatted_content,
                summary=financial_summary,
                tags=financial_tags,
                content_type='enhanced_structured_financial_v3',
                created_at=datetime.utcnow()
            )
            
            db.session.add(generated_content)
            db.session.commit()
            
            logger.info(f"ê°œì„ ëœ êµ¬ì¡°í™”ëœ ê¸ˆìœµ ì½˜í…ì¸  ìƒì„± ì™„ë£Œ: {korean_title}")
            return generated_content
            
        except Exception as e:
            logger.error(f"ê°œì„ ëœ êµ¬ì¡°í™”ëœ ê¸ˆìœµ ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨: {str(e)}")
            db.session.rollback()
            return None
    
    def _clean_and_analyze_content(self, content: str) -> str:
        """ì˜ì–´ ì½˜í…ì¸  ì •ì œ ë° ë¶„ì„ - ê°œì„ ëœ ë²„ì „"""
        # HTML íƒœê·¸ ì œê±°
        content = re.sub(r'<[^>]+>', '', content)
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        content = re.sub(r'\s+', ' ', content)
        
        # ìˆ«ìì™€ í¼ì„¼íŠ¸ íŒ¨í„´ ë³´ì¡´
        content = self._preserve_numerical_data(content)
        
        # ê¸¸ì´ ì œí•œ (ë” ìƒì„¸í•œ ë¶„ì„ì„ ìœ„í•´ ì¦ê°€)
        if len(content) > self.max_length:
            content = content[:self.max_length] + '...'
        
        # ì‹¤ì œ ë²ˆì—­ì€ ì—¬ê¸°ì„œ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì²˜ë¦¬
        # ì‹¤ì œ êµ¬í˜„ì‹œì—ëŠ” Google Translate API ë“± ì‚¬ìš© ê¶Œì¥
        korean_content = self._enhanced_translate(content.strip())
        
        return korean_content
    
    def _preserve_numerical_data(self, content: str) -> str:
        """ìˆ«ì ë°ì´í„° ë³´ì¡´ ë° ê°•ì¡°"""
        import re
        
        # í¼ì„¼íŠ¸, ë‹¬ëŸ¬, ìˆ«ì íŒ¨í„´ ë³´ì¡´
        patterns = [
            (r'(\d+\.?\d*%)', r'ã€\1ã€‘'),  # í¼ì„¼íŠ¸
            (r'(\$\d+\.?\d*[BMK]?)', r'ã€\1ã€‘'),  # ë‹¬ëŸ¬
            (r'(\d+\.?\d*\s*billion)', r'ã€\1ã€‘'),  # ì–µ ë‹¨ìœ„
            (r'(\d+\.?\d*\s*million)', r'ã€\1ã€‘'),  # ë°±ë§Œ ë‹¨ìœ„
            (r'(\d+\.?\d*\s*trillion)', r'ã€\1ã€‘'),  # ì¡° ë‹¨ìœ„
        ]
        
        for pattern, replacement in patterns:
            content = re.sub(pattern, replacement, content, flags=re.IGNORECASE)
        
        return content
    
    def _enhanced_translate(self, content: str) -> str:
        """ê°œì„ ëœ ë²ˆì—­ í•¨ìˆ˜"""
        # ë³´ì¡´ëœ ìˆ«ì ë°ì´í„° ë³µì›
        content = content.replace('ã€', '').replace('ã€‘', '')
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë²ˆì—­ (ì‹¤ì œë¡œëŠ” ë²ˆì—­ API ì‚¬ìš© ê¶Œì¥)
        translations = {
            'Federal Reserve': 'ì—°ë°©ì¤€ë¹„ì œë„',
            'interest rate': 'ê¸ˆë¦¬',
            'inflation': 'ì¸í”Œë ˆì´ì…˜',
            'GDP': 'GDP',
            'stock market': 'ì£¼ì‹ì‹œì¥',
            'recession': 'ê²½ê¸°ì¹¨ì²´',
            'earnings': 'ì‹¤ì ',
            'revenue': 'ë§¤ì¶œ',
            'profit': 'ì´ìµ',
            'investment': 'íˆ¬ì',
            'portfolio': 'í¬íŠ¸í´ë¦¬ì˜¤',
            'volatility': 'ë³€ë™ì„±'
        }
        
        for english, korean in translations.items():
            content = content.replace(english, korean)
        
        return content
    
    def _generate_enhanced_financial_summary(self, content: str) -> str:
        """ê°œì„ ëœ ê¸ˆìœµ ì „ë¬¸ ìš”ì•½ ìƒì„±"""
        # í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ìˆ«ìê°€ í¬í•¨ëœ ë¬¸ì¥ ìš°ì„ )
        sentences = content.split('.')
        important_sentences = []
        
        for sentence in sentences:
            if any(char.isdigit() for char in sentence) or len(sentence.strip()) > 50:
                important_sentences.append(sentence.strip())
                if len(important_sentences) >= 3:
                    break
        
        if not important_sentences:
            important_sentences = sentences[:2]
        
        summary = '. '.join(important_sentences).strip()
        return summary[:500] + '...' if len(summary) > 500 else summary
    
    def _extract_quantitative_financial_tags(self, content: str, title: str) -> List[str]:
        """ì •ëŸ‰ì  ê¸ˆìœµ íƒœê·¸ ì¶”ì¶œ"""
        tags = []
        content_lower = (content + ' ' + title).lower()
        
        # ì •ëŸ‰ì  í‚¤ì›Œë“œ ë§¤í•‘
        quantitative_keywords = {
            # ì‹œì¥ ì§€í‘œ
            'gdp': 'GDP', 'inflation': 'ì¸í”Œë ˆì´ì…˜', 'unemployment': 'ì‹¤ì—…ë¥ ',
            'interest rate': 'ê¸ˆë¦¬', 'yield': 'ìˆ˜ìµë¥ ', 'volatility': 'ë³€ë™ì„±',
            
            # ê¸°ì—… ì‹¤ì 
            'earnings': 'ì‹¤ì ', 'revenue': 'ë§¤ì¶œ', 'profit': 'ì´ìµ', 'eps': 'EPS',
            'dividend': 'ë°°ë‹¹', 'buyback': 'ìì‚¬ì£¼ë§¤ì…',
            
            # ì„¹í„°
            'technology': 'ê¸°ìˆ ì£¼', 'healthcare': 'í—¬ìŠ¤ì¼€ì–´', 'finance': 'ê¸ˆìœµì£¼',
            'energy': 'ì—ë„ˆì§€', 'consumer': 'ì†Œë¹„ì¬', 'industrial': 'ì‚°ì—…ì¬',
            
            # ì•”í˜¸í™”í
            'bitcoin': 'ë¹„íŠ¸ì½”ì¸', 'ethereum': 'ì´ë”ë¦¬ì›€', 'crypto': 'ì•”í˜¸í™”í',
            
            # ì§€ì •í•™
            'fed': 'ì—°ì¤€', 'china': 'ì¤‘êµ­', 'trade war': 'ë¬´ì—­ì „ìŸ',
            'sanctions': 'ì œì¬', 'recession': 'ê²½ê¸°ì¹¨ì²´'
        }
        
        for keyword, tag in quantitative_keywords.items():
            if keyword in content_lower and tag not in tags:
                tags.append(tag)
        
        # ìˆ«ì íŒ¨í„´ ê¸°ë°˜ íƒœê·¸
        import re
        if re.search(r'\d+\.?\d*%', content):
            tags.append('í¼ì„¼íŠ¸ì§€í‘œ')
        if re.search(r'\$\d+', content):
            tags.append('ë‹¬ëŸ¬ìˆ˜ì¹˜')
        if re.search(r'\d+\.?\d*\s*(billion|million|trillion)', content):
            tags.append('ëŒ€ê·œëª¨ìˆ˜ì¹˜')
        
        return tags[:10]  # ìµœëŒ€ 10ê°œ íƒœê·¸
    
    def _translate_title(self, title: str) -> str:
        """ì œëª© ë²ˆì—­"""
        return self._enhanced_translate(title)
    
    def generate_batch_content(self, limit: int = 10) -> List[GeneratedContent]:
        """ë¯¸ì²˜ë¦¬ëœ ê¸ˆìœµ ì½˜í…ì¸ ë“¤ì„ ì¼ê´„ ì²˜ë¦¬"""
        # ì•„ì§ ì½˜í…ì¸ ê°€ ìƒì„±ë˜ì§€ ì•Šì€ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì¡°íšŒ
        unprocessed_contents = db.session.query(ScrapedContent)\
            .outerjoin(GeneratedContent)\
            .filter(GeneratedContent.id == None)\
            .order_by(ScrapedContent.scraped_at.desc())\
            .limit(limit)\
            .all()
        
        generated_contents = []
        
        for scraped_content in unprocessed_contents:
            generated = self.generate_from_scraped_content(scraped_content)
            if generated:
                generated_contents.append(generated)
        
        logger.info(f"ì¼ê´„ ê¸ˆìœµ ì½˜í…ì¸  ìƒì„± ì™„ë£Œ: {len(generated_contents)}ê°œ")
        return generated_contents
    
    def is_financial_content(self, content: str, title: str) -> bool:
        """ê¸ˆìœµ ê´€ë ¨ ì½˜í…ì¸ ì¸ì§€ íŒë‹¨"""
        financial_indicators = [
            'stock', 'market', 'trading', 'investment', 'finance',
            'fed', 'federal reserve', 'interest rate', 'inflation',
            'earnings', 'revenue', 'profit', 'cryptocurrency',
            'bitcoin', 'nasdaq', 's&p', 'dow jones'
        ]
        
        text = f"{title} {content}".lower()
        return any(indicator in text for indicator in financial_indicators)
    
    def _create_structured_content(self, title: str, content: str, source_url: str = None) -> str:
        """êµ¬ì¡°í™”ëœ ê¸ˆìœµ ì½˜í…ì¸  ìƒì„±"""
        
        # 1. ì•„ì´ì½˜ê³¼ ì œëª© ìƒì„±
        emoji_title = self._add_title_emoji(title)
        
        # 2. í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        key_info = self._extract_key_information(content)
        
        # 3. ë°°ê²½/ê²½ìœ„ ì •ë³´ ì¶”ì¶œ
        background_info = self._extract_background_info(content)
        
        # 4. íŒŒê¸‰íš¨ê³¼/ì˜ë¯¸ ì¶”ì¶œ
        impact_info = self._extract_impact_info(content)
        
        # 5. í•œë§ˆë”” ìš”ì•½ ìƒì„±
        one_line_summary = self._generate_one_line_summary(title, content)
        
        # 6. í•´ì‹œíƒœê·¸ ìƒì„±
        hashtags = self._generate_hashtags(title, content)
        
        # êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ì¡°í•©
        structured_content = []
        
        # ë©”ì¸ ì œëª©
        structured_content.append(f"ğŸ‡ºğŸ‡¸ {emoji_title}")
        structured_content.append("")
        structured_content.append("â”" * 40)
        structured_content.append("")
        
        # í•µì‹¬ ì •ë³´ ì„¹ì…˜
        if key_info:
            structured_content.append("ğŸš¨ **í•µì‹¬ ë‚´ìš©**")
            structured_content.append("")
            structured_content.extend(key_info)
            structured_content.append("")
            structured_content.append("â”" * 40)
            structured_content.append("")
        
        # ë°°ê²½/ê²½ìœ„ ì„¹ì…˜
        if background_info:
            structured_content.append("âš¡ **ë°°ê²½ ë° ê²½ìœ„**")
            structured_content.append("")
            structured_content.extend(background_info)
            structured_content.append("")
            structured_content.append("â”" * 40)
            structured_content.append("")
        
        # íŒŒê¸‰íš¨ê³¼/ì˜ë¯¸ ì„¹ì…˜
        if impact_info:
            structured_content.append("ğŸ¯ **íŒŒê¸‰íš¨ê³¼ & ì˜ë¯¸**")
            structured_content.append("")
            structured_content.extend(impact_info)
            structured_content.append("")
            structured_content.append("â”" * 40)
            structured_content.append("")
        
        # í•œë§ˆë”” ìš”ì•½
        structured_content.append(f"**í•œë§ˆë””ë¡œ:** {one_line_summary}")
        structured_content.append("")
        
        # í•´ì‹œíƒœê·¸
        structured_content.append(hashtags)
        
        return "\n".join(structured_content)
    
    def _add_title_emoji(self, title: str) -> str:
        """ì œëª©ì— ì ì ˆí•œ ì´ëª¨ì§€ ì¶”ê°€"""
        title_lower = title.lower()
        
        # í‚¤ì›Œë“œë³„ ì´ëª¨ì§€ ë§¤í•‘
        emoji_map = {
            'ì—°ì¤€': 'ğŸ›ï¸ğŸ’°',
            'ê¸ˆë¦¬': 'ğŸ“ˆğŸ’°',
            'ë¹„íŠ¸ì½”ì¸': 'â‚¿ğŸ’',
            'í…ŒìŠ¬ë¼': 'âš¡ğŸš—',
            'ì• í”Œ': 'ğŸğŸ“±',
            'ì‹¤ì ': 'ğŸ“ŠğŸ’°',
            'ì£¼ì‹': 'ğŸ“ˆğŸ’¹',
            'ì œì¬': 'ğŸ’¥âš“',
            'ì†ë³´': 'ğŸš¨âš¡',
            'ìƒìŠ¹': 'ğŸš€ğŸ“ˆ',
            'í•˜ë½': 'ğŸ“‰ğŸ’¥',
            'ë°œí‘œ': 'ğŸ“¢ğŸ’¼',
        }
        
        # ì ìš©í•  ì´ëª¨ì§€ ì°¾ê¸°
        for keyword, emoji in emoji_map.items():
            if keyword in title:
                return f"{title} {emoji}"
        
        # ê¸°ë³¸ ì´ëª¨ì§€
        return f"{title} ğŸ’°ğŸ“ˆ"
    
    def _extract_key_information(self, content: str) -> List[str]:
        """í•µì‹¬ ì •ë³´ ì¶”ì¶œ"""
        sentences = self._split_sentences(content)
        key_info = []
        
        # ì²« ë²ˆì§¸ ë¬¸ì¥ë¶€í„° ì‹œì‘í•˜ì—¬ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        for sentence in sentences[:6]:  # ì²˜ìŒ 6ê°œ ë¬¸ì¥ ê²€í† 
            if len(sentence.strip()) > 15:
                # ê° ë¬¸ì¥ì„ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬
                formatted = sentence.strip()
                if not formatted.startswith('â€¢'):
                    formatted = f"â€¢ {formatted}"
                key_info.append(formatted)
                
                if len(key_info) >= 4:  # ìµœëŒ€ 4ê°œ í•­ëª©
                    break
        
        return key_info
    
    def _extract_background_info(self, content: str) -> List[str]:
        """ë°°ê²½ ë° ê²½ìœ„ ì •ë³´ ì¶”ì¶œ"""
        sentences = self._split_sentences(content)
        background_info = []
        
        # ì¤‘ê°„ ë¶€ë¶„ ë¬¸ì¥ë“¤ì—ì„œ ë°°ê²½ ì •ë³´ ì¶”ì¶œ
        start_idx = min(2, len(sentences) // 3)  # ì•ë¶€ë¶„ ìŠ¤í‚µ
        
        for sentence in sentences[start_idx:start_idx+4]:
            if len(sentence.strip()) > 15:
                formatted = sentence.strip()
                if not background_info:
                    formatted = f"**ìš´ì†¡ ê²½ìœ„:** {formatted}"
                else:
                    formatted = f"â€¢ {formatted}"
                background_info.append(formatted)
                
                if len(background_info) >= 3:
                    break
        
        return background_info
    
    def _extract_impact_info(self, content: str) -> List[str]:
        """íŒŒê¸‰íš¨ê³¼ ë° ì˜ë¯¸ ì¶”ì¶œ"""
        sentences = self._split_sentences(content)
        impact_info = []
        
        # ë’·ë¶€ë¶„ ë¬¸ì¥ë“¤ì—ì„œ íŒŒê¸‰íš¨ê³¼ ì¶”ì¶œ
        start_idx = max(0, len(sentences) - 4)
        
        for sentence in sentences[start_idx:]:
            if len(sentence.strip()) > 15:
                formatted = sentence.strip()
                if not impact_info:
                    formatted = f"**ì§ì ‘ ì œì¬:** {formatted}"
                else:
                    formatted = f"**2ì°¨ ì œì¬:** {formatted}"
                impact_info.append(formatted)
                
                if len(impact_info) >= 2:
                    break
        
        return impact_info
    
    def _generate_one_line_summary(self, title: str, content: str) -> str:
        """í•œë§ˆë”” ìš”ì•½ ìƒì„±"""
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        sentences = self._split_sentences(content)
        
        # ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ê°€ ë‹´ê¸´ ë¬¸ì¥ ì„ íƒ
        best_sentence = ""
        max_score = 0
        
        important_words = ['ë°œí‘œ', 'ê²°ì •', 'ìƒìŠ¹', 'í•˜ë½', 'í¼ì„¼íŠ¸', '%', 'ì–µ', 'ì¡°']
        
        for sentence in sentences[:5]:  # ì²˜ìŒ 5ê°œ ë¬¸ì¥ë§Œ ê²€í† 
            score = 0
            for word in important_words:
                if word in sentence:
                    score += 1
            
            # ìˆ«ìê°€ í¬í•¨ëœ ê²½ìš° ì¶”ê°€ ì ìˆ˜
            if re.search(r'\d+', sentence):
                score += 2
            
            if score > max_score and len(sentence.strip()) > 10:
                max_score = score
                best_sentence = sentence.strip()
        
        if not best_sentence:
            best_sentence = title
        
        # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°í•˜ê³  ìš”ì•½
        summary = best_sentence.split('.')[0]
        if len(summary) > 100:
            summary = summary[:100] + "..."
        
        return summary
    
    def _generate_hashtags(self, title: str, content: str) -> str:
        """í•´ì‹œíƒœê·¸ ìƒì„±"""
        tags = []
        full_text = f"{title} {content}".lower()
        
        # ì£¼ìš” í•´ì‹œíƒœê·¸ ë§¤í•‘
        hashtag_map = {
            'ì—°ì¤€': '#ì—°ì¤€',
            'ê¸ˆë¦¬': '#ê¸ˆë¦¬',
            'ë¹„íŠ¸ì½”ì¸': '#ë¹„íŠ¸ì½”ì¸',
            'ì•”í˜¸í™”í': '#ì•”í˜¸í™”í',
            'í…ŒìŠ¬ë¼': '#í…ŒìŠ¬ë¼',
            'ì• í”Œ': '#ì• í”Œ',
            'ì‹¤ì ': '#ì‹¤ì ë°œí‘œ',
            'ì£¼ì‹': '#ì£¼ì‹ì‹œì¥',
            'íˆ¬ì': '#ë¯¸êµ­íˆ¬ì',
            'íŠ¸ëŸ¼í”„': '#íŠ¸ëŸ¼í”„',
            'ì œì¬': '#ê²½ì œì œì¬',
            'ì¤‘êµ­': '#ì¤‘êµ­',
            'ì‹œì¥': '#ê¸ˆìœµì‹œì¥'
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword, hashtag in hashtag_map.items():
            if keyword in full_text:
                tags.append(hashtag)
        
        # ê¸°ë³¸ íƒœê·¸ ì¶”ê°€
        if '#ë¯¸êµ­íˆ¬ì' not in tags:
            tags.append('#ë¯¸êµ­íˆ¬ì')
        if '#ê¸ˆìœµë‰´ìŠ¤' not in tags:
            tags.append('#ê¸ˆìœµë‰´ìŠ¤')
        
        return ' '.join(tags[:8])  # ìµœëŒ€ 8ê°œ í•´ì‹œíƒœê·¸
    
    def _split_sentences(self, content: str) -> List[str]:
        """ë¬¸ì¥ ë¶„ë¦¬"""
        # ë¬¸ì¥ êµ¬ë¶„ìë¡œ ë¶„ë¦¬
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]
        return sentences


# ê¸°ì¡´ ContentGeneratorë¥¼ FinancialContentGeneratorë¡œ ëŒ€ì²´
ContentGenerator = FinancialContentGenerator 