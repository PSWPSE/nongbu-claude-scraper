"""
ê¸ˆìœµ/íˆ¬ì ì „ë¬¸ ì½˜í…ì¸  ìƒì„± ì—”ì§„
"""

import logging
import re
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple

from .app import db
from .models import ScrapedContent, GeneratedContent
from .content_templates import get_template, classify_content_type


logger = logging.getLogger(__name__)


class FinancialContentGenerator:
    """ê¸ˆìœµ/íˆ¬ì ì „ë¬¸ ì½˜í…ì¸  ìƒì„±ê¸°"""
    
    def __init__(self):
        self.max_length = 1500
        self.min_length = 100
        
    def generate_from_scraped_content(self, scraped_content: ScrapedContent) -> Optional[GeneratedContent]:
        """ìŠ¤í¬ë˜í•‘ëœ ì½˜í…ì¸ ë¡œë¶€í„° êµ¬ì¡°í™”ëœ ê¸ˆìœµ ì½˜í…ì¸  ìƒì„±"""
        try:
            logger.info(f"êµ¬ì¡°í™”ëœ ê¸ˆìœµ ì½˜í…ì¸  ìƒì„± ì‹œì‘: {scraped_content.title}")
            
            # ì½˜í…ì¸  ì •ì œ ë° ë²ˆì—­
            korean_title = self._translate_title(str(scraped_content.title))
            cleaned_content = self._clean_and_translate_content(str(scraped_content.content))
            
            # ê¸ˆìœµ ì „ë¬¸ ìš”ì•½ ìƒì„±
            financial_summary = self._generate_financial_summary(cleaned_content)
            
            # ê¸ˆìœµ íƒœê·¸ ì¶”ì¶œ
            financial_tags = self._extract_financial_tags(cleaned_content, korean_title)
            
            # ìƒˆë¡œìš´ êµ¬ì¡°í™”ëœ í…œí”Œë¦¿ ì‚¬ìš©
            from .content_templates import StructuredFinancialTemplate
            
            formatted_content = StructuredFinancialTemplate.format_content(
                title=korean_title,
                summary=financial_summary,
                url=str(scraped_content.url) if scraped_content.url else None,
                source=scraped_content.target.name if hasattr(scraped_content, 'target') and scraped_content.target else None,
                tags=financial_tags
            )
            
            # ìƒì„±ëœ ì½˜í…ì¸  ì €ì¥
            generated_content = GeneratedContent(
                scraped_content_id=scraped_content.id,
                title=korean_title,
                content=formatted_content,
                summary=financial_summary,
                tags=financial_tags,
                content_type='structured_financial_v2',
                created_at=datetime.utcnow()
            )
            
            db.session.add(generated_content)
            db.session.commit()
            
            logger.info(f"êµ¬ì¡°í™”ëœ ê¸ˆìœµ ì½˜í…ì¸  ìƒì„± ì™„ë£Œ: {korean_title}")
            return generated_content
            
        except Exception as e:
            logger.error(f"êµ¬ì¡°í™”ëœ ê¸ˆìœµ ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨: {str(e)}")
            db.session.rollback()
            return None
    
    def _clean_and_translate_content(self, content: str) -> str:
        """ì˜ì–´ ì½˜í…ì¸  ì •ì œ ë° í•œêµ­ì–´ ë²ˆì—­"""
        # HTML íƒœê·¸ ì œê±°
        content = re.sub(r'<[^>]+>', '', content)
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        content = re.sub(r'\s+', ' ', content)
        
        # ê¸¸ì´ ì œí•œ
        if len(content) > self.max_length:
            content = content[:self.max_length] + '...'
        
        # ì‹¤ì œ ë²ˆì—­ì€ ì—¬ê¸°ì„œ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì²˜ë¦¬
        # ì‹¤ì œ êµ¬í˜„ì‹œì—ëŠ” Google Translate API ë“± ì‚¬ìš© ê¶Œì¥
        korean_content = self._simple_translate(content.strip())
        
        return korean_content
    
    def _simple_translate(self, text: str) -> str:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë²ˆì—­ (ì‹¤ì œë¡œëŠ” ë²ˆì—­ API ì‚¬ìš© ê¶Œì¥)"""
        # ì£¼ìš” ê¸ˆìœµ ìš©ì–´ ë° ë¬¸ì¥ ë²ˆì—­ ë§¤í•‘
        translation_map = {
            # ê¸ˆìœµ ê¸°ê´€ ë° ì •ë¶€
            'U.S. Treasury Department': 'ë¯¸êµ­ ì¬ë¬´ë¶€',
            'Treasury Department': 'ì¬ë¬´ë¶€',
            'Office of Foreign Assets Control': 'OFAC',
            'OFAC': 'OFAC',
            'Federal Reserve': 'ì—°ì¤€',
            'Fed': 'ì—°ì¤€',
            'Treasury Secretary': 'ì¬ë¬´ì¥ê´€',
            'Secretary': 'ì¥ê´€',
            
            # ê¸ˆìœµ ìš©ì–´
            'sanctions': 'ì œì¬',
            'sanctioned': 'ì œì¬ëœ',
            'designate': 'ì§€ì •',
            'designated': 'ì§€ì •ëœ',
            'blocked': 'ì°¨ë‹¨ëœ',
            'freeze': 'ë™ê²°',
            'assets': 'ìì‚°',
            'transactions': 'ê±°ë˜',
            'companies': 'ê¸°ì—…',
            'individual': 'ê°œì¸',
            'entity': 'ê¸°ì—…ì²´',
            'entities': 'ê¸°ì—…ë“¤',
            
            # êµ­ê°€ ë° ì§€ì—­
            'Chinese': 'ì¤‘êµ­ì¸',
            'China': 'ì¤‘êµ­',
            'Hong Kong': 'í™ì½©',
            'Singapore': 'ì‹±ê°€í¬ë¥´',
            'Iran': 'ì´ë€',
            'Iranian': 'ì´ë€ì˜',
            
            # ì‚°ì—… ê´€ë ¨
            'defense industries': 'êµ­ë°©ì‚°ì—…',
            'defense sector': 'êµ­ë°©ë¶€ë¬¸',
            'machinery': 'ê¸°ê³„ë¥˜',
            'equipment': 'ì¥ë¹„',
            'technology': 'ê¸°ìˆ ',
            'weapons': 'ë¬´ê¸°',
            'arms': 'ë¬´ê¸°',
            'ballistic missile': 'íƒ„ë„ë¯¸ì‚¬ì¼',
            'drone': 'ë¬´ì¸ê¸°',
            'dual-use': 'ì´ì¤‘ìš©ë„',
            
            # ê¸°ì—… ë° ê°œì¸ëª…
            'Zhang Yanbang': 'ì¥ì˜Œë°©(Zhang Yanbang)',
            'Scott Bessent': 'ìŠ¤ì½§ ë² ì„¼íŠ¸',
            'SHUN KAI XING': 'SHUN KAI XING',
            'Unico Shipping Co Ltd': 'í™ì½© ìœ ë‹ˆì½” ì‰¬í•‘(Unico Shipping Co Ltd)',
            'captain': 'ì„ ì¥',
            'vessel': 'ì„ ë°•',
            'shipping': 'ì‰¬í•‘',
            
            # ë™ì‘ ë° ìƒíƒœ
            'announced': 'ë°œí‘œí–ˆë‹¤',
            'targeting': 'ëŒ€ìƒìœ¼ë¡œ',
            'transport': 'ìš´ì†¡',
            'transporting': 'ìš´ì†¡í•˜ë‹¤',
            'attempting': 'ì‹œë„',
            'falsify': 'ìœ„ì¡°',
            'conceal': 'ì€ë‹‰',
            'warned': 'ê²½ê³ í–ˆë‹¤',
            'continue': 'ê³„ì†',
            'blocking': 'ì°¨ë‹¨',
            'procurement': 'ì¡°ë‹¬',
            'supporting': 'ì§€ì›í•˜ëŠ”',
            'conducting': 'ìˆ˜í–‰í•˜ëŠ”',
            'significant': 'ì¤‘ìš”í•œ',
            'face': 'ì§ë©´í• ',
            'aimed at': 'ëª©í‘œë¡œ í•˜ëŠ”',
            'disrupting': 'ì°¨ë‹¨',
            
            # ê¸°íƒ€
            'Friday': 'ê¸ˆìš”ì¼',
            'according to': '~ì— ë”°ë¥´ë©´',
            'Executive Order': 'í–‰ì •ëª…ë ¹',
            'weapons of mass destruction': 'ëŒ€ëŸ‰ì‚´ìƒë¬´ê¸°',
            'proliferation': 'í™•ì‚°',
            'foreign financial institutions': 'ì™¸êµ­ ê¸ˆìœµê¸°ê´€',
            'secondary sanctions': '2ì°¨ ì œì¬',
            'National Security Presidential Memorandum': 'êµ­ê°€ì•ˆë³´ ëŒ€í†µë ¹ ë©”ëª¨ëœë¤'
        }
        
        # ì „ì²´ ë¬¸ì¥ ë²ˆì—­ ë§¤í•‘ (ë” ìì—°ìŠ¤ëŸ¬ìš´ ë²ˆì—­ì„ ìœ„í•´)
        sentence_translations = {
            'The U.S. Treasury Department\'s Office of Foreign Assets Control (OFAC) announced new sanctions Friday targeting one individual and eight companies': 
            'ë¯¸êµ­ ì¬ë¬´ë¶€ OFACê°€ ê¸ˆìš”ì¼ ê°œì¸ 1ëª…ê³¼ ê¸°ì—… 8ê³³ì„ ëŒ€ìƒìœ¼ë¡œ ìƒˆë¡œìš´ ì œì¬ë¥¼ ë°œí‘œí–ˆë‹¤',
            
            'for their role in transporting sensitive machinery to Iran\'s defense industries':
            'ì´ë€ êµ­ë°©ì‚°ì—…ìš© ë¯¼ê° ê¸°ê³„ë¥˜ ìš´ì†¡ì— ê´€ì—¬í•œ í˜ì˜ë¡œ',
            
            'was designated alongside his Hong Kong-based shipping company':
            'í™ì½© ì†Œì¬ ì„ ë°•íšŒì‚¬ì™€ í•¨ê»˜ ì§€ì •ë˜ì—ˆë‹¤',
            
            'The sanctions also target four Chinese companies':
            'ì œì¬ëŠ” ë˜í•œ ì¤‘êµ­ ê¸°ì—… 4ê³³ë„ ëŒ€ìƒìœ¼ë¡œ í•œë‹¤',
            
            'was blocked as a designated asset for attempting to transport dual-use machinery':
            'ì´ì¤‘ìš©ë„ ê¸°ê³„ ìš´ì†¡ ì‹œë„ë¡œ ì°¨ë‹¨ ìì‚°ìœ¼ë¡œ ì§€ì •ë˜ì—ˆë‹¤',
            
            'tried to falsify shipping documents to conceal the Iran-bound cargo':
            'ì´ë€í–‰ í™”ë¬¼ì„ ì€ë‹‰í•˜ê¸° ìœ„í•´ ìš´ì†¡ ì„œë¥˜ ìœ„ì¡°ë¥¼ ì‹œë„í–ˆë‹¤',
            
            'will continue blocking Iran\'s procurement of dual-use technology':
            'ì´ë€ì˜ ì´ì¤‘ìš©ë„ ê¸°ìˆ  ì¡°ë‹¬ì„ ê³„ì† ì°¨ë‹¨í•  ê²ƒì´ë‹¤',
            
            'supporting its ballistic missile, drone, and asymmetric weapons programs':
            'íƒ„ë„ë¯¸ì‚¬ì¼, ë¬´ì¸ê¸°, ë¹„ëŒ€ì¹­ ë¬´ê¸° í”„ë¡œê·¸ë¨ì„ ì§€ì›í•˜ëŠ”',
            
            'may face secondary sanctions':
            '2ì°¨ ì œì¬ì— ì§ë©´í•  ìˆ˜ ìˆë‹¤'
        }
        
        # ë¨¼ì € ì „ì²´ ë¬¸ì¥ ë²ˆì—­ ì‹œë„
        translated_text = text
        for english_sentence, korean_sentence in sentence_translations.items():
            translated_text = translated_text.replace(english_sentence, korean_sentence)
        
        # ê°œë³„ í‚¤ì›Œë“œ ë²ˆì—­
        for english, korean in translation_map.items():
            translated_text = re.sub(rf'\b{re.escape(english)}\b', korean, translated_text, flags=re.IGNORECASE)
        
        return translated_text
    
    def _translate_title(self, title: str) -> str:
        """ì œëª© ë²ˆì—­"""
        return self._simple_translate(title)
    
    def _generate_financial_summary(self, content: str) -> str:
        """ê¸ˆìœµ ì „ë¬¸ ìš”ì•½ ìƒì„±"""
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # ê¸ˆìœµ ê´€ë ¨ ì¤‘ìš” í‚¤ì›Œë“œ
        important_keywords = [
            'ì—°ì¤€', 'ê¸ˆë¦¬', 'ì¸í”Œë ˆì´ì…˜', 'ì‹¤ì ', 'ë§¤ì¶œ', 'ìˆ˜ìµ',
            'ì£¼ì‹', 'íˆ¬ì', 'ì‹œì¥', 'ê±°ë˜', 'ìƒìŠ¹', 'í•˜ë½',
            'ë°œí‘œ', 'ì •ì±…', 'ê²°ì •', 'ì˜ˆìƒ', 'ì „ë§', 'ë¶„ì„',
            'ë¹„íŠ¸ì½”ì¸', 'ì•”í˜¸í™”í', 'íŠ¸ëŸ¼í”„', 'íŒŒì›”'
        ]
        
        # ìˆ«ìë‚˜ í¼ì„¼í‹°ì§€ê°€ í¬í•¨ëœ ë¬¸ì¥ ìš°ì„ 
        priority_sentences = []
        normal_sentences = []
        
        for sentence in sentences[:5]:
            if len(sentence) > 20 and len(sentence) < 200:
                # ìˆ«ì, í¼ì„¼í‹°ì§€, ì¤‘ìš” í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì¥ ìš°ì„ 
                has_numbers = bool(re.search(r'\d+', sentence))
                has_percentage = bool(re.search(r'\d+%', sentence))
                has_keywords = any(keyword in sentence for keyword in important_keywords)
                
                if has_numbers or has_percentage or has_keywords:
                    priority_sentences.append(sentence)
                else:
                    normal_sentences.append(sentence)
        
        # ìš°ì„ ìˆœìœ„ ë¬¸ì¥ë¶€í„° ì„ íƒ
        selected_sentences = priority_sentences[:2] + normal_sentences[:1]
        summary = '. '.join(selected_sentences[:3])
        
        if summary:
            summary += '.'
        else:
            summary = sentences[0] if sentences else content[:200] + '...'
        
        return summary
    
    def _extract_financial_tags(self, content: str, title: str) -> List[str]:
        """ê¸ˆìœµ ì „ë¬¸ íƒœê·¸ ì¶”ì¶œ"""
        tags = []
        full_text = f"{title} {content}".lower()
        
        # ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘
        financial_keywords = {
            'fed': ['ì—°ì¤€', 'federal reserve', 'fed', 'íŒŒì›”', 'powell', 'ê¸ˆë¦¬', 'interest rate'],
            'crypto': ['ë¹„íŠ¸ì½”ì¸', 'bitcoin', 'ì•”í˜¸í™”í', 'cryptocurrency', 'ì´ë”ë¦¬ì›€', 'ethereum', 'crypto'],
            'earnings': ['ì‹¤ì ', 'ìˆ˜ìµ', 'earnings', 'revenue', 'profit', 'ë¶„ê¸°ì‹¤ì '],
            'market': ['ì‹œì¥', 'ì£¼ì‹', 'stock', 'market', 'ê±°ë˜', 'trading'],
            'tech': ['ê¸°ìˆ ì£¼', 'í…Œí¬', 'tech', 'ì• í”Œ', 'apple', 'êµ¬ê¸€', 'google', 'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸', 'microsoft'],
            'trump': ['íŠ¸ëŸ¼í”„', 'trump', 'ëŒ€í†µë ¹', 'president'],
            'breaking': ['ì†ë³´', 'breaking', 'urgent', 'ê¸´ê¸‰'],
            'investment': ['íˆ¬ì', 'investment', 'í€ë“œ', 'fund'],
            'policy': ['ì •ì±…', 'policy', 'ë°œí‘œ', 'announcement'],
            'inflation': ['ì¸í”Œë ˆì´ì…˜', 'inflation', 'ë¬¼ê°€']
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        for tag, keywords in financial_keywords.items():
            if any(keyword in full_text for keyword in keywords):
                tags.append(tag)
        
        # íŠ¹ì • ì¢…ëª©ëª… ê°ì§€
        major_stocks = ['tesla', 'apple', 'microsoft', 'google', 'amazon', 'meta', 'nvidia']
        for stock in major_stocks:
            if stock in full_text:
                tags.append('individual_stock')
                break
        
        # ê¸°ë³¸ íƒœê·¸ ì¶”ê°€
        if not tags:
            tags.append('general_finance')
        
        # í•­ìƒ í¬í•¨í•  ê¸°ë³¸ íƒœê·¸
        if 'market' not in tags:
            tags.append('market')
        
        return tags[:5]  # ìµœëŒ€ 5ê°œ íƒœê·¸
    
    def generate_batch_content(self, limit: int = 10) -> List[GeneratedContent]:
        """ë¯¸ì²˜ë¦¬ëœ ê¸ˆìœµ ì½˜í…ì¸ ë“¤ì„ ì¼ê´„ ì²˜ë¦¬"""
        # ì•„ì§ ì½˜í…ì¸ ê°€ ìƒì„±ë˜ì§€ ì•Šì€ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì¡°íšŒ
        unprocessed_contents = db.session.query(ScrapedContent)\
            .outerjoin(GeneratedContent)\
            .filter(GeneratedContent.id == None)\
            .order_by(ScrapedContent.scraped_at.desc())\
            .limit(limit)\
            .all()
        
        generated_contents = []
        
        for scraped_content in unprocessed_contents:
            generated = self.generate_from_scraped_content(scraped_content)
            if generated:
                generated_contents.append(generated)
        
        logger.info(f"ì¼ê´„ ê¸ˆìœµ ì½˜í…ì¸  ìƒì„± ì™„ë£Œ: {len(generated_contents)}ê°œ")
        return generated_contents
    
    def is_financial_content(self, content: str, title: str) -> bool:
        """ê¸ˆìœµ ê´€ë ¨ ì½˜í…ì¸ ì¸ì§€ íŒë‹¨"""
        financial_indicators = [
            'stock', 'market', 'trading', 'investment', 'finance',
            'fed', 'federal reserve', 'interest rate', 'inflation',
            'earnings', 'revenue', 'profit', 'cryptocurrency',
            'bitcoin', 'nasdaq', 's&p', 'dow jones'
        ]
        
        text = f"{title} {content}".lower()
        return any(indicator in text for indicator in financial_indicators)
    
    def _create_structured_content(self, title: str, content: str, source_url: str = None) -> str:
        """êµ¬ì¡°í™”ëœ ê¸ˆìœµ ì½˜í…ì¸  ìƒì„±"""
        
        # 1. ì•„ì´ì½˜ê³¼ ì œëª© ìƒì„±
        emoji_title = self._add_title_emoji(title)
        
        # 2. í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        key_info = self._extract_key_information(content)
        
        # 3. ë°°ê²½/ê²½ìœ„ ì •ë³´ ì¶”ì¶œ
        background_info = self._extract_background_info(content)
        
        # 4. íŒŒê¸‰íš¨ê³¼/ì˜ë¯¸ ì¶”ì¶œ
        impact_info = self._extract_impact_info(content)
        
        # 5. í•œë§ˆë”” ìš”ì•½ ìƒì„±
        one_line_summary = self._generate_one_line_summary(title, content)
        
        # 6. í•´ì‹œíƒœê·¸ ìƒì„±
        hashtags = self._generate_hashtags(title, content)
        
        # êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ì¡°í•©
        structured_content = []
        
        # ë©”ì¸ ì œëª©
        structured_content.append(f"ğŸ‡ºğŸ‡¸ {emoji_title}")
        structured_content.append("")
        structured_content.append("â”" * 40)
        structured_content.append("")
        
        # í•µì‹¬ ì •ë³´ ì„¹ì…˜
        if key_info:
            structured_content.append("ğŸš¨ **í•µì‹¬ ë‚´ìš©**")
            structured_content.append("")
            structured_content.extend(key_info)
            structured_content.append("")
            structured_content.append("â”" * 40)
            structured_content.append("")
        
        # ë°°ê²½/ê²½ìœ„ ì„¹ì…˜
        if background_info:
            structured_content.append("âš¡ **ë°°ê²½ ë° ê²½ìœ„**")
            structured_content.append("")
            structured_content.extend(background_info)
            structured_content.append("")
            structured_content.append("â”" * 40)
            structured_content.append("")
        
        # íŒŒê¸‰íš¨ê³¼/ì˜ë¯¸ ì„¹ì…˜
        if impact_info:
            structured_content.append("ğŸ¯ **íŒŒê¸‰íš¨ê³¼ & ì˜ë¯¸**")
            structured_content.append("")
            structured_content.extend(impact_info)
            structured_content.append("")
            structured_content.append("â”" * 40)
            structured_content.append("")
        
        # í•œë§ˆë”” ìš”ì•½
        structured_content.append(f"**í•œë§ˆë””ë¡œ:** {one_line_summary}")
        structured_content.append("")
        
        # í•´ì‹œíƒœê·¸
        structured_content.append(hashtags)
        
        return "\n".join(structured_content)
    
    def _add_title_emoji(self, title: str) -> str:
        """ì œëª©ì— ì ì ˆí•œ ì´ëª¨ì§€ ì¶”ê°€"""
        title_lower = title.lower()
        
        # í‚¤ì›Œë“œë³„ ì´ëª¨ì§€ ë§¤í•‘
        emoji_map = {
            'ì—°ì¤€': 'ğŸ›ï¸ğŸ’°',
            'ê¸ˆë¦¬': 'ğŸ“ˆğŸ’°',
            'ë¹„íŠ¸ì½”ì¸': 'â‚¿ğŸ’',
            'í…ŒìŠ¬ë¼': 'âš¡ğŸš—',
            'ì• í”Œ': 'ğŸğŸ“±',
            'ì‹¤ì ': 'ğŸ“ŠğŸ’°',
            'ì£¼ì‹': 'ğŸ“ˆğŸ’¹',
            'ì œì¬': 'ğŸ’¥âš“',
            'ì†ë³´': 'ğŸš¨âš¡',
            'ìƒìŠ¹': 'ğŸš€ğŸ“ˆ',
            'í•˜ë½': 'ğŸ“‰ğŸ’¥',
            'ë°œí‘œ': 'ğŸ“¢ğŸ’¼',
        }
        
        # ì ìš©í•  ì´ëª¨ì§€ ì°¾ê¸°
        for keyword, emoji in emoji_map.items():
            if keyword in title:
                return f"{title} {emoji}"
        
        # ê¸°ë³¸ ì´ëª¨ì§€
        return f"{title} ğŸ’°ğŸ“ˆ"
    
    def _extract_key_information(self, content: str) -> List[str]:
        """í•µì‹¬ ì •ë³´ ì¶”ì¶œ"""
        sentences = self._split_sentences(content)
        key_info = []
        
        # ì²« ë²ˆì§¸ ë¬¸ì¥ë¶€í„° ì‹œì‘í•˜ì—¬ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        for sentence in sentences[:6]:  # ì²˜ìŒ 6ê°œ ë¬¸ì¥ ê²€í† 
            if len(sentence.strip()) > 15:
                # ê° ë¬¸ì¥ì„ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬
                formatted = sentence.strip()
                if not formatted.startswith('â€¢'):
                    formatted = f"â€¢ {formatted}"
                key_info.append(formatted)
                
                if len(key_info) >= 4:  # ìµœëŒ€ 4ê°œ í•­ëª©
                    break
        
        return key_info
    
    def _extract_background_info(self, content: str) -> List[str]:
        """ë°°ê²½ ë° ê²½ìœ„ ì •ë³´ ì¶”ì¶œ"""
        sentences = self._split_sentences(content)
        background_info = []
        
        # ì¤‘ê°„ ë¶€ë¶„ ë¬¸ì¥ë“¤ì—ì„œ ë°°ê²½ ì •ë³´ ì¶”ì¶œ
        start_idx = min(2, len(sentences) // 3)  # ì•ë¶€ë¶„ ìŠ¤í‚µ
        
        for sentence in sentences[start_idx:start_idx+4]:
            if len(sentence.strip()) > 15:
                formatted = sentence.strip()
                if not background_info:
                    formatted = f"**ìš´ì†¡ ê²½ìœ„:** {formatted}"
                else:
                    formatted = f"â€¢ {formatted}"
                background_info.append(formatted)
                
                if len(background_info) >= 3:
                    break
        
        return background_info
    
    def _extract_impact_info(self, content: str) -> List[str]:
        """íŒŒê¸‰íš¨ê³¼ ë° ì˜ë¯¸ ì¶”ì¶œ"""
        sentences = self._split_sentences(content)
        impact_info = []
        
        # ë’·ë¶€ë¶„ ë¬¸ì¥ë“¤ì—ì„œ íŒŒê¸‰íš¨ê³¼ ì¶”ì¶œ
        start_idx = max(0, len(sentences) - 4)
        
        for sentence in sentences[start_idx:]:
            if len(sentence.strip()) > 15:
                formatted = sentence.strip()
                if not impact_info:
                    formatted = f"**ì§ì ‘ ì œì¬:** {formatted}"
                else:
                    formatted = f"**2ì°¨ ì œì¬:** {formatted}"
                impact_info.append(formatted)
                
                if len(impact_info) >= 2:
                    break
        
        return impact_info
    
    def _generate_one_line_summary(self, title: str, content: str) -> str:
        """í•œë§ˆë”” ìš”ì•½ ìƒì„±"""
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        sentences = self._split_sentences(content)
        
        # ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ê°€ ë‹´ê¸´ ë¬¸ì¥ ì„ íƒ
        best_sentence = ""
        max_score = 0
        
        important_words = ['ë°œí‘œ', 'ê²°ì •', 'ìƒìŠ¹', 'í•˜ë½', 'í¼ì„¼íŠ¸', '%', 'ì–µ', 'ì¡°']
        
        for sentence in sentences[:5]:  # ì²˜ìŒ 5ê°œ ë¬¸ì¥ë§Œ ê²€í† 
            score = 0
            for word in important_words:
                if word in sentence:
                    score += 1
            
            # ìˆ«ìê°€ í¬í•¨ëœ ê²½ìš° ì¶”ê°€ ì ìˆ˜
            if re.search(r'\d+', sentence):
                score += 2
            
            if score > max_score and len(sentence.strip()) > 10:
                max_score = score
                best_sentence = sentence.strip()
        
        if not best_sentence:
            best_sentence = title
        
        # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°í•˜ê³  ìš”ì•½
        summary = best_sentence.split('.')[0]
        if len(summary) > 100:
            summary = summary[:100] + "..."
        
        return summary
    
    def _generate_hashtags(self, title: str, content: str) -> str:
        """í•´ì‹œíƒœê·¸ ìƒì„±"""
        tags = []
        full_text = f"{title} {content}".lower()
        
        # ì£¼ìš” í•´ì‹œíƒœê·¸ ë§¤í•‘
        hashtag_map = {
            'ì—°ì¤€': '#ì—°ì¤€',
            'ê¸ˆë¦¬': '#ê¸ˆë¦¬',
            'ë¹„íŠ¸ì½”ì¸': '#ë¹„íŠ¸ì½”ì¸',
            'ì•”í˜¸í™”í': '#ì•”í˜¸í™”í',
            'í…ŒìŠ¬ë¼': '#í…ŒìŠ¬ë¼',
            'ì• í”Œ': '#ì• í”Œ',
            'ì‹¤ì ': '#ì‹¤ì ë°œí‘œ',
            'ì£¼ì‹': '#ì£¼ì‹ì‹œì¥',
            'íˆ¬ì': '#ë¯¸êµ­íˆ¬ì',
            'íŠ¸ëŸ¼í”„': '#íŠ¸ëŸ¼í”„',
            'ì œì¬': '#ê²½ì œì œì¬',
            'ì¤‘êµ­': '#ì¤‘êµ­',
            'ì‹œì¥': '#ê¸ˆìœµì‹œì¥'
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword, hashtag in hashtag_map.items():
            if keyword in full_text:
                tags.append(hashtag)
        
        # ê¸°ë³¸ íƒœê·¸ ì¶”ê°€
        if '#ë¯¸êµ­íˆ¬ì' not in tags:
            tags.append('#ë¯¸êµ­íˆ¬ì')
        if '#ê¸ˆìœµë‰´ìŠ¤' not in tags:
            tags.append('#ê¸ˆìœµë‰´ìŠ¤')
        
        return ' '.join(tags[:8])  # ìµœëŒ€ 8ê°œ í•´ì‹œíƒœê·¸
    
    def _split_sentences(self, content: str) -> List[str]:
        """ë¬¸ì¥ ë¶„ë¦¬"""
        # ë¬¸ì¥ êµ¬ë¶„ìë¡œ ë¶„ë¦¬
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]
        return sentences


# ê¸°ì¡´ ContentGeneratorë¥¼ FinancialContentGeneratorë¡œ ëŒ€ì²´
ContentGenerator = FinancialContentGenerator 