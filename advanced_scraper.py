"""
ê³ ê¸‰ ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œ
- ë‹¤ì¤‘ ê¸°ìˆ  ìŠ¤íƒ í™œìš© (requests, Selenium, Newspaper3k, Trafilatura)
- ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ íŠ¹í™”
- ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ í•„í„°ë§
- ì•ˆí‹°-ìŠ¤í¬ë˜í•‘ ìš°íšŒ ê¸°ëŠ¥
"""

import asyncio
import hashlib
import logging
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple, Set
import re
from urllib.parse import urljoin, urlparse
import sqlite3
import os

import requests
from bs4 import BeautifulSoup
import newspaper
from newspaper import Article
import trafilatura
from readability import Document
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
DB_PATHS = [
    'src/instance/nongbu_financial.db',
    'instance/nongbu_financial_clean.db',
    'nongbu_financial.db'
]

# ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ (í•œêµ­ íˆ¬ìì ê´€ì‹¬ì‚¬)
FINANCIAL_KEYWORDS = {
    'stocks': ['stock', 'shares', 'equity', 'market', 'trading', 'nasdaq', 'dow', 's&p', 'russell'],
    'companies': ['apple', 'tesla', 'microsoft', 'amazon', 'google', 'nvidia', 'meta', 'berkshire'],
    'crypto': ['bitcoin', 'cryptocurrency', 'crypto', 'ethereum', 'blockchain', 'defi'],
    'fed': ['federal reserve', 'fed', 'powell', 'interest rate', 'monetary policy', 'fomc'],
    'economy': ['inflation', 'gdp', 'employment', 'economic', 'recession', 'growth'],
    'earnings': ['earnings', 'revenue', 'profit', 'quarterly', 'financial results'],
    'geopolitics': ['trade war', 'china', 'tariff', 'sanction', 'oil', 'energy']
}

# User-Agent í’€ (ì°¨ë‹¨ ë°©ì§€)
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'
]

# ì‚¬ì´íŠ¸ë³„ ë§ì¶¤ ì„¤ì •
SITE_CONFIGS = {
    'bbc.com': {'delay_range': (1, 3), 'timeout': 20},
    'apnews.com': {'delay_range': (2, 4), 'timeout': 25},
    'npr.org': {'delay_range': (1, 2), 'timeout': 15},
    'finviz.com': {'delay_range': (1, 3), 'timeout': 15},
    'finance.yahoo.com': {'delay_range': (2, 5), 'timeout': 20}
}


class AdvancedScraper:
    """ê³ ê¸‰ ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œ"""
    
    def __init__(self, use_selenium=False):
        self.use_selenium = use_selenium
        self.driver = None
        self.session = self._create_session()
        
        # ìš”ì²­ ê°„ê²© (ì´ˆ)
        self.request_delay = 2
        
        # ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ ì‚¬ì´íŠ¸ (ì‘ë™ í™•ì¸ëœ ì‚¬ì´íŠ¸ë“¤)
        self.targets = {
            'BBC Business': {
                'url': 'https://www.bbc.com/business',
                'article_selectors': ['article a', '.gs-c-promo-heading a', 'h3 a', 'h2 a'],
                'keywords': ['stocks', 'companies', 'economy', 'earnings'],
                'use_selenium': False,  # requests ìš°ì„  ì‚¬ìš©
                'content_type': 'news'
            },
            'AP News Business': {
                'url': 'https://apnews.com/hub/business',
                'article_selectors': ['h2 a', 'h3 a', '.PagePromo-title a', 'article a'],
                'keywords': ['stocks', 'companies', 'crypto', 'earnings'],
                'use_selenium': False,  # requests ìš°ì„  ì‚¬ìš©
                'content_type': 'news'
            },
            'NPR Business': {
                'url': 'https://www.npr.org/sections/business/',
                'article_selectors': ['h2 a', 'h3 a', '.item-info-wrap a', 'article a'],
                'keywords': ['stocks', 'fed', 'economy', 'earnings'],
                'use_selenium': False,  # requests ìš°ì„  ì‚¬ìš©
                'content_type': 'news'
            },
            'FINVIZ': {
                'url': 'https://finviz.com/news.ashx',
                'article_selectors': ['.news-link-container a', '.news-link a', 'td a[target="_blank"]'],
                'keywords': ['stocks', 'companies', 'earnings', 'market'],
                'use_selenium': False,
                'content_type': 'financial_data'
            },
            'Yahoo Finance': {
                'url': 'https://finance.yahoo.com/news/',
                'article_selectors': ['h3 a', 'h2 a', '.js-content-viewer a', '[data-module="stream"] a'],
                'keywords': ['stocks', 'companies', 'earnings', 'market', 'fed'],
                'use_selenium': False,
                'content_type': 'financial_news'
            }
        }
    
    def _create_session(self) -> requests.Session:
        """HTTP ì„¸ì…˜ ìƒì„± - ê°œì„ ëœ ì°¨ë‹¨ ë°©ì§€ ê¸°ëŠ¥"""
        session = requests.Session()
        
        # ëœë¤ User-Agent ì„ íƒ
        user_agent = random.choice(USER_AGENTS)
        
        session.headers.update({
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none'
        })
        return session
    
    def _get_selenium_driver(self) -> webdriver.Chrome:
        """Selenium WebDriver ì´ˆê¸°í™”"""
        if self.driver is None:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--window-size=1920,1080')
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            
            self.driver = webdriver.Chrome(
                service=Service(ChromeDriverManager().install()),
                options=options
            )
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        return self.driver
    
    def get_db_connection(self) -> sqlite3.Connection:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        for db_path in DB_PATHS:
            if os.path.exists(db_path):
                return sqlite3.connect(db_path)
        raise FileNotFoundError("ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def extract_with_multiple_methods(self, url: str) -> str:
        """ë‹¤ì¤‘ ë°©ë²•ì„ ì‚¬ìš©í•œ ë³¸ë¬¸ ì¶”ì¶œ"""
        logger.info(f"ë‹¤ì¤‘ ë°©ë²•ìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘: {url}")
        
        best_content = ""
        best_score = 0
        
        # ë°©ë²• 1: Trafilatura (ê°€ì¥ ê°•ë ¥í•œ ë³¸ë¬¸ ì¶”ì¶œ ë„êµ¬)
        try:
            content = trafilatura.fetch_url(url)
            if content:
                extracted = trafilatura.extract(content, include_comments=False, include_tables=False)
                if extracted and len(extracted) > best_score:
                    best_content = extracted
                    best_score = len(extracted)
                    logger.info(f"Trafilaturaë¡œ {len(extracted)}ì ì¶”ì¶œ")
        except Exception as e:
            logger.warning(f"Trafilatura ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 2: Newspaper3k (ë‰´ìŠ¤ íŠ¹í™”)
        try:
            article = Article(url)
            article.download()
            article.parse()
            if article.text and len(article.text) > best_score:
                best_content = article.text
                best_score = len(article.text)
                logger.info(f"Newspaper3kë¡œ {len(article.text)}ì ì¶”ì¶œ")
        except Exception as e:
            logger.warning(f"Newspaper3k ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 3: Readability (ê°€ë…ì„± ì¤‘ì‹¬ ì¶”ì¶œ)
        try:
            response = self.session.get(url, timeout=15)
            if response.status_code == 200:
                doc = Document(response.content)
                content = BeautifulSoup(doc.summary(), 'html.parser').get_text(strip=True)
                if content and len(content) > best_score:
                    best_content = content
                    best_score = len(content)
                    logger.info(f"Readabilityë¡œ {len(content)}ì ì¶”ì¶œ")
        except Exception as e:
            logger.warning(f"Readability ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 4: Selenium (JavaScript ë Œë”ë§ í•„ìš”í•œ ê²½ìš°)
        if self.use_selenium and best_score < 500:
            try:
                content = self._extract_with_selenium(url)
                if content and len(content) > best_score:
                    best_content = content
                    best_score = len(content)
                    logger.info(f"Seleniumìœ¼ë¡œ {len(content)}ì ì¶”ì¶œ")
            except Exception as e:
                logger.warning(f"Selenium ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 5: ì»¤ìŠ¤í…€ BeautifulSoup (ê¸°ë³¸ ë°©ë²•)
        if best_score < 300:
            try:
                content = self._extract_with_beautifulsoup(url)
                if content and len(content) > best_score:
                    best_content = content
                    best_score = len(content)
                    logger.info(f"BeautifulSoupìœ¼ë¡œ {len(content)}ì ì¶”ì¶œ")
            except Exception as e:
                logger.warning(f"BeautifulSoup ì‹¤íŒ¨: {e}")
        
        logger.info(f"ìµœì¢… ì¶”ì¶œ ê²°ê³¼: {best_score}ì")
        return best_content
    
    def _extract_with_selenium(self, url: str) -> str:
        """Seleniumì„ ì‚¬ìš©í•œ ë³¸ë¬¸ ì¶”ì¶œ"""
        driver = self._get_selenium_driver()
        driver.get(url)
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        time.sleep(3)  # ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        return self._extract_content_from_soup(soup)
    
    def _extract_with_beautifulsoup(self, url: str) -> str:
        """BeautifulSoupì„ ì‚¬ìš©í•œ ë³¸ë¬¸ ì¶”ì¶œ"""
        response = self.session.get(url, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        return self._extract_content_from_soup(soup)
    
    def _extract_content_from_soup(self, soup: BeautifulSoup) -> str:
        """BeautifulSoup ê°ì²´ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for unwanted in soup.select('script, style, nav, header, footer, .ad, .advertisement, .social-share, .related-articles'):
            unwanted.decompose()
        
        # ë‹¤ì–‘í•œ ë³¸ë¬¸ ì…€ë ‰í„° ì‹œë„
        content_selectors = [
            'article', '.article', '.content', '.post-content', '.entry-content',
            '.story-body', '.article-body', '.body-text', '.text-content',
            '[data-module="ArticleBody"]', '.article__content', '.story-content',
            'main .content', '.main-content', '.primary-content'
        ]
        
        best_content = ""
        best_length = 0
        
        for selector in content_selectors:
            elements = soup.select(selector)
            for element in elements:
                content = element.get_text(separator='\n', strip=True)
                if len(content) > best_length:
                    best_content = content
                    best_length = len(content)
        
        # paragraph ë°©ì‹ë„ ì‹œë„
        if best_length < 500:
            paragraphs = soup.select('p')
            meaningful_paragraphs = [
                p.get_text(strip=True) for p in paragraphs 
                if len(p.get_text(strip=True)) > 30
            ]
            if meaningful_paragraphs:
                paragraph_content = '\n'.join(meaningful_paragraphs)
                if len(paragraph_content) > best_length:
                    best_content = paragraph_content
        
        return best_content
    
    def is_relevant_content(self, title: str, content: str) -> bool:
        """ì½˜í…ì¸ ì˜ ê¸ˆìœµ ê´€ë ¨ì„± ë° í’ˆì§ˆ ê²€ì‚¬"""
        text = (title + " " + content).lower()
        
        # 1. ì½˜í…ì¸  ê¸¸ì´ ê²€ì‚¬ (ìµœì†Œ 300ì ì´ìƒ)
        if len(content) < 300:
            logger.info(f"âš ï¸ ì½˜í…ì¸ ê°€ ë„ˆë¬´ ì§§ìŒ: {len(content)}ì")
            return False
        
        # 2. ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒ¨í„´ ê²€ì‚¬ (ì˜ë¯¸ì—†ëŠ” ì½˜í…ì¸  ì°¨ë‹¨)
        blacklist_patterns = [
            # íšŒì‚¬ ì†Œê°œê¸€ íŒ¨í„´
            r'the associated press is an independent global news organization',
            r'founded in \d{4}',
            r'remains the most trusted source',
            r'more than half the world\'s population sees',
            r'essential provider of the technology',
            r'vital to the news business',
            
            # ì¼ë°˜ì ì¸ ì†Œê°œ íŒ¨í„´  
            r'subscribe to our newsletter',
            r'follow us on social media',
            r'contact us for more information',
            r'copyright \d{4}',
            r'all rights reserved',
            r'privacy policy',
            r'terms of service',
            r'cookie policy',
            
            # ë„¤ë¹„ê²Œì´ì…˜/ë©”ë‰´ í…ìŠ¤íŠ¸
            r'home\s+news\s+business',
            r'breaking news',
            r'latest news',
            r'trending now',
            
            # ë„ˆë¬´ ì¼ë°˜ì ì¸ ë¬¸êµ¬ë“¤
            r'click here to',
            r'read more about',
            r'learn more at',
            r'visit our website',
        ]
        
        for pattern in blacklist_patterns:
            if re.search(pattern, text):
                logger.info(f"âš ï¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒ¨í„´ ê°ì§€: {pattern}")
                return False
        
        # 3. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        relevance_score = 0
        for category, keywords in FINANCIAL_KEYWORDS.items():
            for keyword in keywords:
                if keyword in text:
                    relevance_score += 1
        
        # 4. ê¸°ë³¸ ê¸ˆìœµ ìš©ì–´ ì²´í¬
        basic_financial_terms = [
            'stock', 'market', 'price', 'trade', 'investment', 'investor',
            'financial', 'economy', 'economic', 'revenue', 'profit', 'loss',
            'nasdaq', 'dow jones', 'sp500', 'wall street', 'trading', 'earnings',
            'billion', 'million', 'percent', '%', 'shares', 'ceo', 'quarterly'
        ]
        
        for term in basic_financial_terms:
            if term in text:
                relevance_score += 1
        
        # 5. ì œëª©ì˜ ê¸ˆìœµ ê´€ë ¨ì„± ê°•í™” ì ìˆ˜
        title_score = 0
        title_lower = title.lower()
        for term in basic_financial_terms:
            if term in title_lower:
                title_score += 2  # ì œëª©ì— ìˆìœ¼ë©´ ê°€ì¤‘ì¹˜ 2ë°°
        
        total_score = relevance_score + title_score
        
        # 6. ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜ (êµ¬ì²´ì  ìˆ«ìë‚˜ ë‚ ì§œ í¬í•¨)
        quality_patterns = [
            r'\$\d+(?:\.\d+)?(?:\s*(?:billion|million|trillion))?',  # ë‹¬ëŸ¬ ê¸ˆì•¡
            r'\d+(?:\.\d+)?%',  # í¼ì„¼íŠ¸
            r'\d{4}-\d{2}-\d{2}',  # ë‚ ì§œ
            r'q[1-4]\s+\d{4}',  # ë¶„ê¸° ì •ë³´
        ]
        
        quality_score = 0
        for pattern in quality_patterns:
            if re.search(pattern, text):
                quality_score += 1
        
        final_score = total_score + quality_score
        min_score = 6  # ìµœì†Œ 4ì  ì´ìƒ í•„ìš”
        
        logger.info(f"ì½˜í…ì¸  ì ìˆ˜: ê´€ë ¨ì„±={relevance_score}, ì œëª©={title_score}, í’ˆì§ˆ={quality_score}, ì´í•©={final_score}/{min_score}")
        return final_score >= min_score
    
    def scrape_all_targets(self) -> Dict[str, Any]:
        """ëª¨ë“  íƒ€ê²Ÿ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
        results = {
            'total_found': 0,
            'total_new': 0,
            'target_results': []
        }
        
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        for target_name, config in self.targets.items():
            try:
                logger.info(f"ğŸš€ {target_name} ìŠ¤í¬ë˜í•‘ ì‹œì‘")
                target_result = self._scrape_target(target_name, config, cursor)
                results['target_results'].append(target_result)
                results['total_found'] += target_result['found']
                results['total_new'] += target_result['new']
                
                # ìš”ì²­ ê°„ê²© ì¤€ìˆ˜
                self.adaptive_delay(config['url'])
                
            except Exception as e:
                logger.error(f"âŒ {target_name} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                results['target_results'].append({
                    'target': target_name,
                    'status': 'failed',
                    'error': str(e),
                    'found': 0,
                    'new': 0
                })
        
        conn.commit()
        conn.close()
        return results
    
    def _scrape_target(self, target_name: str, config: Dict[str, Any], cursor) -> Dict[str, Any]:
        """ê°œë³„ íƒ€ê²Ÿ ìŠ¤í¬ë˜í•‘"""
        url = config['url']
        article_selectors = config['article_selectors']
        
        logger.info(f"íƒ€ê²Ÿ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {target_name} -> {url}")
        
        # ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
        try:
            # ê°œë³„ íƒ€ê²Ÿì˜ Selenium ì‚¬ìš© ì—¬ë¶€ í™•ì¸ (ê¸°ë³¸ê°’ì€ False)
            target_use_selenium = config.get('use_selenium', False)
            
            if target_use_selenium:
                logger.info("Seleniumì„ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ë¡œë“œ ì¤‘...")
                driver = self._get_selenium_driver()
                driver.get(url)
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(3)
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                logger.info(f"í˜ì´ì§€ ì†ŒìŠ¤ ê¸¸ì´: {len(driver.page_source)}ì")
            else:
                logger.info("requestsë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ìš”ì²­ ì¤‘...")
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                logger.info(f"ì‘ë‹µ ë‚´ìš© ê¸¸ì´: {len(response.content)}ì")
        
        except Exception as e:
            logger.error(f"í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {str(e)}")
            return {
                'target': target_name,
                'status': 'failed',
                'error': f'ë©”ì¸ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {str(e)}',
                'found': 0,
                'new': 0
            }
        
        # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
        logger.info("ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ ì‹œì‘...")
        article_links = self._extract_article_links(soup, article_selectors, url)
        
        found_count = len(article_links)
        new_count = 0
        
        logger.info(f"ğŸ“° {target_name}ì—ì„œ {found_count}ê°œ ê¸°ì‚¬ ë§í¬ ë°œê²¬")
        
        if found_count == 0:
            logger.warning("ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡° í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return {
                'target': target_name,
                'status': 'failed',
                'error': 'ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ',
                'found': 0,
                'new': 0
            }
        
        # ê° ê¸°ì‚¬ì˜ ë³¸ë¬¸ ì¶”ì¶œ
        for i, (article_title, article_url) in enumerate(article_links[:3]):  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 3ê°œë§Œ
            try:
                logger.info(f"ğŸ“– ê¸°ì‚¬ {i+1}/{min(len(article_links), 3)}: {article_title[:50]}...")
                logger.info(f"ê¸°ì‚¬ URL: {article_url}")
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content = self.extract_with_multiple_methods(article_url)
                
                if not content or len(content) < 100:
                    logger.warning(f"âš ï¸ ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ: {len(content) if content else 0}ì")
                    continue
                
                # ê´€ë ¨ì„± ê²€ì‚¬
                if not self.is_relevant_content(article_title, content):
                    logger.info(f"â­ï¸ ê¸ˆìœµ ê´€ë ¨ì„± ë‚®ìŒ, ê±´ë„ˆëœ€")
                    continue
                
                # ì¤‘ë³µ ê²€ì‚¬
                content_hash = hashlib.sha256((article_title + content).encode('utf-8')).hexdigest()
                cursor.execute("SELECT COUNT(*) FROM scraped_contents WHERE content_hash = ?", (content_hash,))
                if cursor.fetchone()[0] > 0:
                    logger.info(f"â­ï¸ ì¤‘ë³µ ì½˜í…ì¸ , ê±´ë„ˆëœ€")
                    continue
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                cursor.execute("""
                    INSERT INTO scraped_contents (title, content, url, content_hash, scraped_at, target_id)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    article_title, 
                    content, 
                    article_url, 
                    content_hash, 
                    datetime.now().isoformat(), 
                    1  # ê¸°ë³¸ target_id
                ))
                
                new_count += 1
                logger.info(f"âœ… ìƒˆ ì½˜í…ì¸  ì €ì¥: {len(content)}ì")
                
                # ìš”ì²­ ê°„ê²© ì¤€ìˆ˜
                self.adaptive_delay(article_url)
                
            except Exception as e:
                logger.error(f"âš ï¸ ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue
        
        return {
            'target': target_name,
            'status': 'success',
            'found': found_count,
            'new': new_count
        }
    
    def _extract_article_links(self, soup: BeautifulSoup, selectors: List[str], base_url: str) -> List[Tuple[str, str]]:
        """ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ"""
        links = []
        
        logger.info(f"ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ ì‹œì‘ - ì‹œë„í•  ì…€ë ‰í„°: {selectors}")
        
        for selector in selectors:
            logger.info(f"ì…€ë ‰í„° ì‹œë„: {selector}")
            elements = soup.select(selector)
            logger.info(f"ë°œê²¬ëœ ìš”ì†Œ ìˆ˜: {len(elements)}")
            
            for i, element in enumerate(elements[:10]):  # ì²˜ìŒ 10ê°œë§Œ í™•ì¸
                try:
                    # ë§í¬ì™€ ì œëª© ì¶”ì¶œ
                    if element.name == 'a':
                        link_elem = element
                        title = element.get_text(strip=True)
                    else:
                        link_elem = element.find('a')
                        title = element.get_text(strip=True)
                    
                    if not link_elem or not title:
                        logger.debug(f"ìš”ì†Œ {i}: ë§í¬ ë˜ëŠ” ì œëª© ì—†ìŒ")
                        continue
                    
                    href = link_elem.get('href')
                    if not href:
                        logger.debug(f"ìš”ì†Œ {i}: href ì†ì„± ì—†ìŒ")
                        continue
                    
                    # hrefê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                    if not isinstance(href, str):
                        logger.debug(f"ìš”ì†Œ {i}: hrefê°€ ë¬¸ìì—´ì´ ì•„ë‹˜: {type(href)}")
                        continue
                    
                    # ì ˆëŒ€ URLë¡œ ë³€í™˜
                    full_url = urljoin(base_url, href)
                    
                    # ìœ íš¨í•œ URLì¸ì§€ í™•ì¸
                    if not full_url.startswith('http'):
                        logger.debug(f"ìš”ì†Œ {i}: ìœ íš¨í•˜ì§€ ì•Šì€ URL: {full_url}")
                        continue
                    
                    # ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ê²½ìš° ì œì™¸
                    if len(title) < 10 or len(title) > 200:
                        logger.debug(f"ìš”ì†Œ {i}: ì œëª© ê¸¸ì´ ë¶€ì ì ˆ: {len(title)}ì")
                        continue
                    
                    logger.info(f"ìœ íš¨í•œ ë§í¬ ë°œê²¬: {title[:50]}... -> {full_url}")
                    links.append((title, full_url))
                    
                except Exception as e:
                    logger.warning(f"ìš”ì†Œ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            if links:
                logger.info(f"ì…€ë ‰í„° '{selector}'ë¡œ {len(links)}ê°œ ë§í¬ ë°œê²¬")
                break  # ë§í¬ë¥¼ ì°¾ì•˜ìœ¼ë©´ ë‹¤ë¥¸ ì…€ë ‰í„°ëŠ” ì‹œë„í•˜ì§€ ì•ŠìŒ
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_links = []
        for title, url in links:
            if url not in seen:
                seen.add(url)
                unique_links.append((title, url))
        
        logger.info(f"ì´ {len(unique_links)}ê°œ ê³ ìœ  ë§í¬ ì¶”ì¶œ ì™„ë£Œ")
        return unique_links
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            self.driver.quit()
            self.driver = None
        if self.session:
            self.session.close()
    
    def get_site_config(self, url: str) -> Dict[str, Any]:
        """URLì—ì„œ ì‚¬ì´íŠ¸ë³„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
        domain = urlparse(url).netloc.lower()
        for site, config in SITE_CONFIGS.items():
            if site in domain:
                return config
        # ê¸°ë³¸ ì„¤ì •
        return {'delay_range': (2, 4), 'timeout': 20}
    
    def adaptive_delay(self, url: str) -> None:
        """ì‚¬ì´íŠ¸ë³„ ì ì‘í˜• ë”œë ˆì´"""
        config = self.get_site_config(url)
        min_delay, max_delay = config['delay_range']
        delay = random.uniform(min_delay, max_delay)
        logger.info(f"â±ï¸ {delay:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
        time.sleep(delay)
    
    def save_to_database(self, title: str, content: str, url: str, source: str = "Manual Collection") -> bool:
        """ìŠ¤í¬ë˜í•‘ëœ ì½˜í…ì¸ ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ - ê°•í™”ëœ ë©”íƒ€ë°ì´í„° í¬í•¨"""
        try:
            # DB ì—°ê²° ê²½ë¡œë“¤
            db_paths = [
                'src/instance/nongbu_financial.db',  # ìš°ì„  ê²½ë¡œ
                'instance/nongbu_financial.db',
                'nongbu_financial.db'
            ]
            
            db_path = None
            for path in db_paths:
                if os.path.exists(path):
                    db_path = path
                    break
            
            if not db_path:
                logger.error("ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = self._generate_content_metadata(title, content, source)
            
            with sqlite3.connect(db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO scraped_contents (title, content, url, source, created_at, metadata)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (title, content, url, source, datetime.now().isoformat(), metadata))
                conn.commit()
                logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {title[:50]}...")
                return True
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _generate_content_metadata(self, title: str, content: str, source: str) -> str:
        """ì½˜í…ì¸  ë©”íƒ€ë°ì´í„° ìƒì„±"""
        text = (title + " " + content).lower()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ë¶„ì„
        keyword_matches = {}
        total_score = 0
        
        for category, keywords in FINANCIAL_KEYWORDS.items():
            category_score = sum(1 for keyword in keywords if keyword in text)
            if category_score > 0:
                keyword_matches[category] = category_score
                total_score += category_score
        
        # ì†ŒìŠ¤ íƒ€ì… ë¶„ë¥˜
        source_type = "unknown"
        reliability = "medium"
        
        if "bbc" in source.lower():
            source_type = "news_major"
            reliability = "high"
        elif "reuters" in source.lower() or "ap news" in source.lower():
            source_type = "news_wire"
            reliability = "high"
        elif "finviz" in source.lower() or "yahoo finance" in source.lower():
            source_type = "financial_data"
            reliability = "medium"
        elif "npr" in source.lower():
            source_type = "news_public"
            reliability = "high"
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì¡°í™”
        metadata = {
            "relevance_score": total_score,
            "keyword_matches": keyword_matches,
            "source_type": source_type,
            "reliability": reliability,
            "content_length": len(content),
            "processed_at": datetime.now().isoformat()
        }
        
        return str(metadata)  # JSON ë¬¸ìì—´ë¡œ ì €ì¥


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = AdvancedScraper(use_selenium=False)
    
    try:
        logger.info("ğŸš€ ê³ ê¸‰ ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œ ì‹œì‘")
        results = scraper.scrape_all_targets()
        
        logger.info("ğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼:")
        logger.info(f"ì´ ë°œê²¬: {results['total_found']}ê°œ")
        logger.info(f"ìƒˆë¡œ ìˆ˜ì§‘: {results['total_new']}ê°œ")
        
        for target_result in results['target_results']:
            status = "âœ…" if target_result['status'] == 'success' else "âŒ"
            logger.info(f"{status} {target_result['target']}: ë°œê²¬ {target_result['found']}ê°œ, ìƒˆë¡œìš´ {target_result['new']}ê°œ")
        
        return results
        
    except Exception as e:
        logger.error(f"ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return {'error': str(e)}
    finally:
        scraper.close()


if __name__ == "__main__":
    main() 